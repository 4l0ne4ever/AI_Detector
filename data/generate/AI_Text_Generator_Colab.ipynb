{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Enhanced AI Text Generator for Google Colab\n",
        "\n",
        "A streamlined AI text generator with quality controls for scientific papers.\n",
        "\n",
        "**Features:**\n",
        "- Length control (±20% of original)\n",
        "- Topic consistency validation\n",
        "- Real-time quality metrics\n",
        "- Resume capability\n",
        "- GPU acceleration\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload your `human_text_50k.jsonl` file\n",
        "2. Run all cells in order\n",
        "3. Download the generated `ai_generated_colab.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers torch accelerate datasets tqdm\n",
        "\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n",
        "import gc\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_data"
      },
      "outputs": [],
      "source": [
        "# Upload your human text data file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your human_text_50k.jsonl file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify upload\n",
        "input_file = 'human_text_50k.jsonl'\n",
        "if input_file in uploaded:\n",
        "    print(f\"File uploaded: {input_file}\")\n",
        "    with open(input_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"Total texts to process: {len(lines):,}\")\n",
        "else:\n",
        "    print(\"Please upload the human_text_50k.jsonl file\")\n",
        "    raise FileNotFoundError(\"Data file not uploaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality_controller"
      },
      "outputs": [],
      "source": [
        "# Quality Controller Class\n",
        "class QualityController:\n",
        "    \"\"\"Handles quality control functions for AI text generation\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:\n",
        "        \"\"\"Extract key terms from text for topic consistency checking\"\"\"\n",
        "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
        "        word_freq = {}\n",
        "        stop_words = {'this', 'that', 'with', 'from', 'they', 'have', 'been', \n",
        "                     'will', 'were', 'said', 'using', 'approach', 'method'}\n",
        "        \n",
        "        for word in words:\n",
        "            if word not in stop_words:\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "        \n",
        "        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:max_keywords]\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_topic_consistency(original: str, generated: str, title: str, threshold: float = 0.3) -> bool:\n",
        "        \"\"\"Check if generated text maintains topic consistency\"\"\"\n",
        "        original_keywords = set(QualityController.extract_keywords(original))\n",
        "        generated_keywords = set(QualityController.extract_keywords(generated))\n",
        "        title_keywords = set(QualityController.extract_keywords(title))\n",
        "        \n",
        "        if len(original_keywords) == 0:\n",
        "            return True\n",
        "        \n",
        "        # Check overlap with original text\n",
        "        overlap = len(original_keywords.intersection(generated_keywords))\n",
        "        consistency_score = overlap / len(original_keywords)\n",
        "        \n",
        "        # Check overlap with title\n",
        "        title_overlap = len(title_keywords.intersection(generated_keywords))\n",
        "        title_consistency = title_overlap / max(len(title_keywords), 1)\n",
        "        \n",
        "        return consistency_score >= threshold or title_consistency >= threshold\n",
        "    \n",
        "    @staticmethod\n",
        "    def calculate_target_length(original_length: int, tolerance: float = 0.2) -> Tuple[int, int]:\n",
        "        \"\"\"Calculate target length range for generated text\"\"\"\n",
        "        min_length = int(original_length * (1 - tolerance))\n",
        "        max_length = int(original_length * (1 + tolerance))\n",
        "        return min_length, max_length\n",
        "\n",
        "print(\"Quality Controller loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_model"
      },
      "outputs": [],
      "source": [
        "# Setup AI Model for Generation\n",
        "MODEL_NAME = \"gpt2-large\"  # High quality generation, better length matching\n",
        "# Alternative: \"gpt2-medium\" (faster) or \"microsoft/DialoGPT-large\" (conversational)\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load model and tokenizer with GPU support\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model with proper device handling\n",
        "if torch.cuda.is_available():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    # Setup generation pipeline (no device argument when using device_map)\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    # Setup generation pipeline for CPU\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=-1\n",
        "    )\n",
        "\n",
        "# Add padding token if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Check model device\n",
        "if hasattr(model, 'device'):\n",
        "    print(f\"Model device: {model.device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    print(\"Model device: GPU (auto-mapped)\")\n",
        "else:\n",
        "    print(\"Model device: CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai_generator"
      },
      "outputs": [],
      "source": [
        "# AI Generator Class\n",
        "class ColabAIGenerator:\n",
        "    \"\"\"Colab-optimized AI text generator\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.quality_controller = QualityController()\n",
        "    \n",
        "    def create_prompts(self, title: str, abstract: str) -> List[str]:\n",
        "        \"\"\"Create enhanced prompts for AI generation\"\"\"\n",
        "        original_length = len(abstract)\n",
        "        min_length, max_length = self.quality_controller.calculate_target_length(original_length)\n",
        "        \n",
        "        prompts = [\n",
        "            f\"\"\"Rewrite this scientific abstract using different academic language while maintaining the same topic and approximate length ({min_length}-{max_length} characters):\n",
        "\n",
        "Title: {title}\n",
        "Original Abstract: {abstract}\n",
        "\n",
        "Rewritten Abstract:\"\"\",\n",
        "            \n",
        "            f\"\"\"Create an alternative version of this scientific abstract. Keep the same research topic and maintain similar length ({min_length}-{max_length} characters):\n",
        "\n",
        "Title: {title}\n",
        "Original: {abstract}\n",
        "\n",
        "Alternative Version:\"\"\",\n",
        "            \n",
        "            f\"\"\"Generate a new version of this scientific abstract using different wording but the same research focus. Target length: {min_length}-{max_length} characters.\n",
        "\n",
        "Title: {title}\n",
        "Reference: {abstract}\n",
        "\n",
        "New Abstract:\"\"\"\n",
        "        ]\n",
        "        \n",
        "        return prompts\n",
        "    \n",
        "    def generate_ai_version(self, title: str, abstract: str, max_attempts: int = 3) -> str:\n",
        "        \"\"\"Generate AI version with quality controls\"\"\"\n",
        "        original_length = len(abstract)\n",
        "        prompts = self.create_prompts(title, abstract)\n",
        "        \n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                prompt = random.choice(prompts)\n",
        "                set_seed(random.randint(1, 10000))\n",
        "                \n",
        "                # Calculate dynamic token length based on original text\n",
        "                # Use tokenizer for accurate token count\n",
        "                orig_tokens = len(tokenizer.encode(abstract, add_special_tokens=False))\n",
        "                \n",
        "                # Target tokens with ±20% tolerance, clamped to reasonable bounds\n",
        "                # Respect model's 1024 token limit\n",
        "                prompt_tokens = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
        "                available_tokens = 1020 - prompt_tokens  # Leave buffer for model limit\n",
        "                target_tokens = min(orig_tokens, available_tokens - 20)\n",
        "                target_tokens = max(80, min(500, target_tokens))  # Higher bounds for gpt2-large\n",
        "                \n",
        "                min_new_tokens = max(60, int(target_tokens * 0.9))  # Target 90% length\n",
        "                max_new_tokens = min(available_tokens - 5, int(target_tokens * 1.1))  # Allow 10% over\n",
        "                \n",
        "                result = generator(\n",
        "                    prompt,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    min_new_tokens=min_new_tokens,\n",
        "                    temperature=random.uniform(0.7, 1.0),\n",
        "                    top_p=random.uniform(0.85, 0.95),\n",
        "                    top_k=random.randint(40, 80),\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    num_return_sequences=1,\n",
        "                    repetition_penalty=1.2\n",
        "                )\n",
        "                \n",
        "                generated_text = result[0]['generated_text']\n",
        "                \n",
        "                # Extract generated part (after prompt)\n",
        "                ai_text = generated_text[len(prompt):].strip()\n",
        "                \n",
        "                # Clean up artifacts\n",
        "                ai_text = re.sub(r'^(rewritten abstract:|alternative version:|new abstract:)\\s*', \n",
        "                                '', ai_text, flags=re.IGNORECASE)\n",
        "                ai_text = re.sub(r'\\[ai-generated.*?\\]', '', ai_text, flags=re.IGNORECASE)\n",
        "                ai_text = ai_text.replace('\\n', ' ').strip()\n",
        "                \n",
        "                # Length adjustment\n",
        "                if len(ai_text) > original_length * 2.0:\n",
        "                    ai_text = ai_text[:int(original_length * 1.5)]\n",
        "                elif len(ai_text) < max(50, original_length * 0.3):\n",
        "                    continue  # Only reject if extremely short\n",
        "                \n",
        "                # Quality check\n",
        "                if self.quality_controller.check_topic_consistency(abstract, ai_text, title):\n",
        "                    return ai_text\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Generation attempt {attempt + 1} failed: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Enhanced fallback\n",
        "        fallback_length = min(300, original_length)\n",
        "        title_clean = title.lower().replace(':', '').strip()\n",
        "        abstract_excerpt = abstract[:fallback_length]\n",
        "        \n",
        "        return f\"This research on {title_clean} investigates {abstract_excerpt}... [Enhanced AI-generated summary maintaining original research focus]\"\n",
        "\n",
        "# Initialize generator\n",
        "ai_gen = ColabAIGenerator()\n",
        "print(\"AI Generator initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_processing"
      },
      "outputs": [],
      "source": [
        "# Data Processing Functions\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data loading and processing\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_human_texts(file_path: str) -> List[Dict]:\n",
        "        \"\"\"Load human texts from JSONL file\"\"\"\n",
        "        texts = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                try:\n",
        "                    entry = json.loads(line.strip())\n",
        "                    texts.append(entry)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Skipping invalid JSON on line {line_num}: {e}\")\n",
        "                    continue\n",
        "        return texts\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_title_and_text(entry: Dict) -> Tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"Extract title and text from entry\"\"\"\n",
        "        title = entry.get('metadata', {}).get('title', entry.get('title'))\n",
        "        text = entry.get('text', '')\n",
        "        return title, text\n",
        "    \n",
        "    @staticmethod\n",
        "    def save_ai_entry(f, ai_text: str, original_entry: Dict, \n",
        "                     original_length: int, generated_length: int) -> None:\n",
        "        \"\"\"Save AI-generated entry to file\"\"\"\n",
        "        length_ratio = generated_length / original_length if original_length > 0 else 0\n",
        "        \n",
        "        ai_entry = {\n",
        "            'text': ai_text,\n",
        "            'label': 'ai',\n",
        "            'source': 'enhanced_colab_generation',\n",
        "            'original_id': original_entry.get('metadata', {}).get('arxiv_id', 'unknown'),\n",
        "            'original_length': original_length,\n",
        "            'generated_length': generated_length,\n",
        "            'length_ratio': round(length_ratio, 2),\n",
        "            'generated_at': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        f.write(json.dumps(ai_entry, ensure_ascii=False) + '\\n')\n",
        "        f.flush()\n",
        "\n",
        "processor = DataProcessor()\n",
        "print(\"Data Processor initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_and_analyze_data"
      },
      "outputs": [],
      "source": [
        "# Load and analyze human texts\n",
        "print(\"Loading human texts...\")\n",
        "human_texts = processor.load_human_texts('human_text_50k.jsonl')\n",
        "print(f\"Loaded {len(human_texts):,} human texts\")\n",
        "\n",
        "# Analyze data structure\n",
        "if human_texts:\n",
        "    sample = human_texts[0]\n",
        "    print(f\"\\nData Structure Analysis:\")\n",
        "    print(f\"   Entry keys: {list(sample.keys())}\")\n",
        "    \n",
        "    if 'metadata' in sample:\n",
        "        print(f\"   Metadata keys: {list(sample['metadata'].keys())}\")\n",
        "        title = sample['metadata'].get('title', 'N/A')\n",
        "        text_length = len(sample.get('text', ''))\n",
        "        \n",
        "        print(f\"\\nSample Entry:\")\n",
        "        print(f\"   Title: {title[:80]}...\")\n",
        "        print(f\"   Text length: {text_length:,} characters\")\n",
        "        print(f\"   Source: {sample.get('source', 'N/A')}\")\n",
        "\n",
        "# Calculate average length\n",
        "lengths = [len(entry.get('text', '')) for entry in human_texts[:1000]]\n",
        "avg_length = sum(lengths) / len(lengths)\n",
        "print(f\"Average text length (sample): {avg_length:.0f} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation_settings"
      },
      "outputs": [],
      "source": [
        "# Generation Settings - Parallel Processing Friendly\n",
        "\n",
        "# PARALLEL PROCESSING CONFIGURATION\n",
        "# Set these values for each Colab instance:\n",
        "INSTANCE_ID = 1        # Change this: 1, 2, 3, etc. for each instance\n",
        "TOTAL_INSTANCES = 3    # Total number of parallel instances\n",
        "TARGET_TOTAL = 15000   # Total texts to generate across all instances (5k each)\n",
        "\n",
        "# Calculate range for this instance\n",
        "texts_per_instance = TARGET_TOTAL // TOTAL_INSTANCES\n",
        "start_offset = (INSTANCE_ID - 1) * texts_per_instance\n",
        "end_offset = min(start_offset + texts_per_instance, len(human_texts))\n",
        "\n",
        "# Override batch settings for parallel processing\n",
        "BATCH_SIZE_ENTRIES = texts_per_instance\n",
        "MAX_GENERATE = texts_per_instance\n",
        "\n",
        "print(f\"Parallel Processing Info:\")\n",
        "print(f\"   Instance ID: {INSTANCE_ID} of {TOTAL_INSTANCES}\")\n",
        "print(f\"   Total dataset: {len(human_texts):,} entries\")\n",
        "print(f\"   This instance range: {start_offset:,} to {end_offset:,}\")\n",
        "print(f\"   This instance will generate: {texts_per_instance:,} texts\")\n",
        "print(f\"   Estimated time: {texts_per_instance/2.5/60:.1f} hours\")\n",
        "GPU_BATCH_SIZE = 4 if torch.cuda.is_available() else 2\n",
        "SAVE_EVERY = 50  # Save more frequently for better resume\n",
        "OUTPUT_FILE = 'ai_generated_colab.jsonl'\n",
        "\n",
        "print(f\"\\nGeneration Settings:\")\n",
        "print(f\"   This session will generate: {MAX_GENERATE:,} entries\")\n",
        "print(f\"   GPU batch size: {GPU_BATCH_SIZE}\")\n",
        "print(f\"   Save progress every: {SAVE_EVERY} entries\")\n",
        "print(f\"   Output file: {OUTPUT_FILE}\")\n",
        "\n",
        "# Check for existing progress\n",
        "start_idx = 0\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    with open(OUTPUT_FILE, 'r') as f:\n",
        "        start_idx = len(f.readlines())\n",
        "    print(f\"Resuming from index {start_idx:,}\")\n",
        "\n",
        "total_to_generate = min(MAX_GENERATE, len(human_texts) - start_idx)\n",
        "print(f\"Will generate: {total_to_generate:,} new texts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_generation"
      },
      "outputs": [],
      "source": [
        "# Main Generation Loop\n",
        "if total_to_generate <= 0:\n",
        "    print(\"Generation already complete!\")\n",
        "else:\n",
        "    print(f\"Starting AI text generation...\")\n",
        "    print(f\"Expected time: ~{total_to_generate/20:.0f} minutes with GPU\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    generated_count = 0\n",
        "    quality_stats = {'length_matches': 0, 'topic_consistent': 0, 'fallback_used': 0}\n",
        "    \n",
        "    mode = 'a' if start_idx > 0 else 'w'\n",
        "    with open(OUTPUT_FILE, mode, encoding='utf-8') as f:\n",
        "        # Use parallel processing range\n",
        "        actual_start = start_offset + start_idx\n",
        "        actual_end = min(actual_start + total_to_generate, end_offset)\n",
        "        \n",
        "        for i in tqdm(range(actual_start, actual_end), desc=f\"Instance {INSTANCE_ID} generating\"):\n",
        "            entry = human_texts[i]\n",
        "            \n",
        "            try:\n",
        "                title, text = processor.extract_title_and_text(entry)\n",
        "                \n",
        "                if not title or not text:\n",
        "                    print(f\"\\nMissing title or text for entry {i}, skipping...\")\n",
        "                    continue\n",
        "                \n",
        "                # Generate AI version\n",
        "                ai_text = ai_gen.generate_ai_version(title, text)\n",
        "                \n",
        "                # Quality assessment\n",
        "                original_length = len(text)\n",
        "                ai_length = len(ai_text)\n",
        "                length_ratio = ai_length / original_length if original_length > 0 else 0\n",
        "                \n",
        "                if 0.8 <= length_ratio <= 1.2:\n",
        "                    quality_stats['length_matches'] += 1\n",
        "                \n",
        "                if ai_gen.quality_controller.check_topic_consistency(text, ai_text, title):\n",
        "                    quality_stats['topic_consistent'] += 1\n",
        "                \n",
        "                if '[Enhanced AI-generated' in ai_text:\n",
        "                    quality_stats['fallback_used'] += 1\n",
        "                \n",
        "                # Save entry\n",
        "                processor.save_ai_entry(f, ai_text, entry, original_length, ai_length)\n",
        "                generated_count += 1\n",
        "                \n",
        "                # Progress update\n",
        "                if generated_count % SAVE_EVERY == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    rate = generated_count / elapsed * 60\n",
        "                    length_match_pct = quality_stats['length_matches'] / generated_count * 100\n",
        "                    topic_match_pct = quality_stats['topic_consistent'] / generated_count * 100\n",
        "                    \n",
        "                    # Calculate overall progress including previous sessions\n",
        "                    total_completed = start_idx + generated_count\n",
        "                    overall_progress = total_completed / len(human_texts) * 100\n",
        "                    \n",
        "                    print(f\"\\nSession Progress: {generated_count}/{total_to_generate}\")\n",
        "                    print(f\"Overall Progress: {total_completed:,}/{len(human_texts):,} ({overall_progress:.1f}%)\")\n",
        "                    print(f\"Rate: {rate:.1f} texts/minute\")\n",
        "                    print(f\"Quality: {length_match_pct:.1f}% length match, {topic_match_pct:.1f}% topic consistent\")\n",
        "                    \n",
        "                    # Memory cleanup\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\nError with entry {i}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    # Final statistics\n",
        "    total_time = time.time() - start_time\n",
        "    final_rate = generated_count / total_time * 60\n",
        "    \n",
        "    print(f\"\\nSession Complete!\")\n",
        "    print(f\"Generated this session: {generated_count:,} texts\")\n",
        "    print(f\"Final rate: {final_rate:.1f} texts/minute\")\n",
        "    print(f\"Session time: {total_time/60:.1f} minutes\")\n",
        "    \n",
        "    # Calculate overall progress\n",
        "    total_completed = start_idx + generated_count\n",
        "    overall_progress = total_completed / len(human_texts) * 100\n",
        "    remaining = len(human_texts) - total_completed\n",
        "    sessions_left = (remaining + BATCH_SIZE_ENTRIES - 1) // BATCH_SIZE_ENTRIES\n",
        "    \n",
        "    print(f\"\\nOverall Progress:\")\n",
        "    print(f\"   Completed: {total_completed:,}/{len(human_texts):,} ({overall_progress:.1f}%)\")\n",
        "    print(f\"   Remaining: {remaining:,} texts\")\n",
        "    if sessions_left > 0:\n",
        "        print(f\"   Sessions left: {sessions_left}\")\n",
        "    else:\n",
        "        print(f\"   ALL SESSIONS COMPLETE!\")\n",
        "    print(f\"   Saved to: {OUTPUT_FILE}\")\n",
        "    \n",
        "    # Quality summary\n",
        "    if generated_count > 0:\n",
        "        length_match_pct = quality_stats['length_matches'] / generated_count * 100\n",
        "        topic_consistent_pct = quality_stats['topic_consistent'] / generated_count * 100\n",
        "        fallback_pct = quality_stats['fallback_used'] / generated_count * 100\n",
        "        \n",
        "        print(f\"\\nFinal Quality Summary:\")\n",
        "        print(f\"   Length matching (±20%): {length_match_pct:.1f}%\")\n",
        "        print(f\"   Topic consistency: {topic_consistent_pct:.1f}%\")\n",
        "        print(f\"   Fallback used: {fallback_pct:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# Download Results\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    # Show final stats\n",
        "    with open(OUTPUT_FILE, 'r') as f:\n",
        "        ai_texts = f.readlines()\n",
        "    \n",
        "    print(f\"Final Statistics:\")\n",
        "    print(f\"   Human texts: {len(human_texts):,}\")\n",
        "    print(f\"   AI texts generated: {len(ai_texts):,}\")\n",
        "    print(f\"   File size: {os.path.getsize(OUTPUT_FILE) / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    # Show sample AI text\n",
        "    if ai_texts:\n",
        "        sample_ai = json.loads(ai_texts[0])\n",
        "        print(f\"\\nSample AI text:\")\n",
        "        print(f\"   {sample_ai['text'][:200]}...\")\n",
        "        print(f\"   Length ratio: {sample_ai['length_ratio']}\")\n",
        "    \n",
        "    print(f\"\\nDownloading {OUTPUT_FILE}...\")\n",
        "    files.download(OUTPUT_FILE)\n",
        "    print(\"Download complete!\")\n",
        "    \n",
        "    print(f\"\\nNext Steps:\")\n",
        "    print(f\"   1. Download completed: {OUTPUT_FILE}\")\n",
        "    print(f\"   2. Use this file for your training dataset\")\n",
        "    print(f\"   3. Combine with human texts for balanced dataset\")\n",
        "    \n",
        "else:\n",
        "    print(f\"Output file not found: {OUTPUT_FILE}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}