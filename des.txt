# AI-Generated Text Detection System
## Architecture & Workflow Documentation

---

## 1. System Overview

### 1.1 Core Objective
Build an end-to-end pipeline that:
- Collects authentic scientific text (human-written)
- Generates synthetic scientific text (AI-generated)
- Trains multiple classification models
- Evaluates detection performance across different model architectures

### 1.2 Key Design Principles
- **Data Quality First**: Ensure both human and AI corpora are realistic and high-quality
- **Fair Comparison**: Use identical preprocessing for all models
- **Reproducibility**: Document all decisions, parameters, and random seeds
- **Scalability**: Design for 100K samples initially, expandable to millions

---

## 2. Data Architecture

### 2.1 Human Text Collection Pipeline

**Source**: S2ORC (Semantic Scholar Open Research Corpus)

**Collection Strategy**:
```
Input: S2ORC API/Download
  ↓
Filter by Criteria:
  • Publication year: 2020-2024 (recent style)
  • Language: English only
  • Length: 100-500 words (for abstracts)
  • Quality: Citation count ≥ 5 OR venue rank A/B
  • Fields: Balanced distribution across domains
  ↓
Extract:
  • Abstract (primary)
  • Introduction paragraphs (secondary)
  ↓
Quality Control:
  • Remove excessive equations/formulas
  • Remove non-textual content
  • Check for language quality
  ↓
Output: human_corpus.jsonl (100K samples)
  Format: {id, text, domain, year, venue}
```

**Domain Distribution Strategy**:
- Computer Science: 25%
- Biology/Medicine: 25%
- Physics/Engineering: 25%
- Social Sciences: 15%
- Mathematics: 10%

**Rationale**: 
- Recent papers (2020-2024) reflect modern academic writing style
- Balanced domains prevent model from learning domain-specific patterns instead of AI vs human patterns
- Quality filters ensure professional writing standards

### 2.2 AI Text Generation Pipeline

**Multi-LLM Strategy**:
```
For each human text sample:
  ↓
Extract metadata:
  • Domain/field
  • Key terms/concepts
  • Approximate length
  ↓
Select LLM (random):
  • GPT-4 / GPT-3.5 (30%)
  • Claude (20%)
  • LLaMA-3 (20%)
  • Qwen-2.5 (20%)
  • Gemini (10%)
  ↓
Select prompt template (random):
  • Template A: "Write an academic abstract about [topic]"
  • Template B: "Summarize research on [topic] in scientific style"
  • Template C: "Generate a paragraph discussing [concept]"
  ↓
Generate with variation:
  • Temperature: randomly sampled from [0.7, 1.0]
  • Top-p: 0.9
  • Max tokens: match human text length ±20%
  ↓
Output: ai_corpus.jsonl (100K samples)
  Format: {id, text, source_llm, temperature, prompt_template}
```

**Rationale**:
- **Multiple LLMs**: Real-world scenario involves various AI tools, not just one
- **Prompt variation**: Prevents model from learning prompt-specific artifacts
- **Temperature variation**: Creates diversity in generation style (deterministic vs creative)
- **Length matching**: Ensures model doesn't learn "AI text is always exactly 200 words"

### 2.3 Dataset Structure

**Final Dataset Schema**:
```
Combined Corpus (200K samples):
├── train_set (70%): 140K samples
│   ├── human: 70K
│   └── ai: 70K
├── validation_set (15%): 30K samples
│   ├── human: 15K
│   └── ai: 15K
└── test_set (15%): 30K samples
    ├── human: 15K
    └── ai: 15K

Stratification:
- Balanced human/AI ratio in each split
- Balanced domain distribution in each split
- Balanced LLM sources in AI portion
```

**Special Test Sets** (for robustness evaluation):
- **Cross-domain test**: Train on CS, test on Biology
- **Adversarial test**: Human text post-edited by Grammarly, AI text post-edited by humans
- **Mixed test**: Paragraphs that are 50% human-written, 50% AI-completed
- **New LLM test**: Text from LLMs not seen during training (e.g., newly released models)

---

## 3. Core Architecture

### 3.1 Preprocessing Pipeline

```
Raw Text
  ↓
Text Cleaning:
  • Remove URLs, email addresses
  • Normalize whitespace
  • Handle special characters (preserve scientific notation)
  • Remove excessive punctuation
  ↓
Linguistic Normalization:
  • Lowercasing (optional, test both)
  • Tokenization
  • Stopword handling (optional, compare performance)
  ↓
Branch A: Classical ML Path
  ↓
Feature Engineering:
  • Statistical features:
    - Avg sentence length, word length
    - Vocabulary richness (TTR)
    - Punctuation density
  • N-gram features:
    - Unigram, bigram, trigram frequencies
    - Character n-grams
  • Linguistic features:
    - POS tag distribution
    - Dependency tree depth
    - Readability scores (Flesch-Kincaid)
  • Perplexity-based features:
    - Perplexity from GPT-2
    - Burstiness score
  • Semantic features:
    - TF-IDF vectors
    - Word2Vec/GloVe embeddings (averaged)
  ↓
Feature Matrix: (n_samples, n_features)

Branch B: Deep Learning Path
  ↓
Tokenization:
  • BERT WordPiece tokenizer
  • Max length: 512 tokens
  • Padding/truncation
  ↓
Token IDs + Attention Masks
```

**Key Decision Points**:
- **Lowercasing**: Test both ways; might remove signal (e.g., AI overuses proper capitalization)
- **Stopwords**: Keep them initially; they may contain style signals
- **Feature selection**: Use mutual information or ANOVA F-test to select top features

### 3.2 Model Architecture Tiers

#### **Tier 1: Classical ML Baseline**

**Logistic Regression**:
```
Architecture:
  Input: Feature vector (e.g., 10K dimensions from TF-IDF)
  ↓
  Linear combination: w₁x₁ + w₂x₂ + ... + wₙxₙ + b
  ↓
  Sigmoid activation: σ(z) = 1 / (1 + e^(-z))
  ↓
  Output: P(AI-generated)

Hyperparameters:
  • Regularization: L2 (Ridge), test C ∈ {0.01, 0.1, 1, 10}
  • Solver: 'lbfgs' for large datasets
  • Class weight: balanced (to handle any imbalance)

Advantages:
  • Fast training
  • Interpretable coefficients
  • Good baseline for comparison

Expected Performance: 75-85% accuracy
```

**Support Vector Machine (SVM)**:
```
Architecture:
  Input: Feature vector
  ↓
  Find optimal hyperplane: max margin separation
  ↓
  Decision function: sign(w·x + b)
  ↓
  Output: Binary classification

Hyperparameters:
  • Kernel: RBF vs Linear (compare both)
  • C: {0.1, 1, 10} (regularization)
  • Gamma: 'scale' vs 'auto' (for RBF)

Advantages:
  • Robust to high dimensions
  • Effective with clear margin of separation

Expected Performance: 80-88% accuracy
```

**Random Forest**:
```
Architecture:
  Input: Feature vector
  ↓
  Build ensemble of decision trees:
    • Bootstrap sampling
    • Random feature subsets
    • Each tree votes
  ↓
  Aggregate votes (majority voting)
  ↓
  Output: Classification + confidence

Hyperparameters:
  • n_estimators: {100, 200, 500}
  • max_depth: {10, 20, None}
  • min_samples_split: {2, 5, 10}
  • Feature importance tracking

Advantages:
  • Handles non-linear patterns
  • Feature importance analysis
  • Robust to outliers

Expected Performance: 82-90% accuracy
```

#### **Tier 2: Neural Network Baseline**

**Multi-Layer Perceptron (MLP)**:
```
Architecture:
  Input Layer: Feature vector (n_features)
  ↓
  Hidden Layer 1: Dense(512, activation='relu')
  Dropout(0.3)
  ↓
  Hidden Layer 2: Dense(256, activation='relu')
  Dropout(0.3)
  ↓
  Hidden Layer 3: Dense(128, activation='relu')
  Dropout(0.2)
  ↓
  Output Layer: Dense(1, activation='sigmoid')

Training:
  • Optimizer: Adam (lr=0.001)
  • Loss: Binary cross-entropy
  • Batch size: 64
  • Epochs: 20-50 (with early stopping)
  • Regularization: L2 + Dropout

Advantages:
  • Learns non-linear feature combinations
  • More powerful than linear models
  • Bridge between classical ML and transformers

Expected Performance: 85-92% accuracy
```

#### **Tier 3: Transformer-Based Models**

**BERT Fine-tuning**:
```
Architecture:
  Pre-trained BERT-base-uncased (110M parameters)
  ↓
  [CLS] token representation
  ↓
  Dropout(0.1)
  ↓
  Linear classifier: Dense(2)
  ↓
  Softmax → [P(human), P(AI)]

Fine-tuning Strategy:
  • Freeze first 6 layers (keep general language knowledge)
  • Fine-tune layers 7-12 + classifier
  • Learning rate: 2e-5 (small for fine-tuning)
  • Warmup steps: 10% of total steps
  • Batch size: 16 (with gradient accumulation if needed)
  • Epochs: 3-5

Advantages:
  • Contextual understanding
  • Captures subtle linguistic patterns
  • State-of-the-art baseline

Expected Performance: 90-96% accuracy
```

**SciBERT (Domain-Specific)**:
```
Same architecture as BERT, but:
  • Pre-trained on scientific papers (Semantic Scholar corpus)
  • Better vocabulary for scientific terms
  • Expected 2-4% improvement over vanilla BERT

Use case: Primary model for scientific text
```

**DistilBERT (Efficient Alternative)**:
```
Same architecture as BERT, but:
  • 40% smaller, 60% faster
  • 95% of BERT's performance
  
Use case: Production deployment where speed matters
```

### 3.3 Ensemble Strategy

**Why Ensemble?**
Different models capture different signals:
- Classical ML: Surface-level statistics
- MLP: Non-linear feature combinations
- BERT: Deep contextual patterns

**Ensemble Architecture**:
```
Input Text
  ↓
  ├─→ Model 1 (SVM) → P₁(AI)
  ├─→ Model 2 (RF)  → P₂(AI)
  ├─→ Model 3 (MLP) → P₃(AI)
  └─→ Model 4 (BERT) → P₄(AI)
  ↓
Aggregation Methods (test all):
  • Voting: Majority vote (threshold=0.5)
  • Averaging: (P₁ + P₂ + P₃ + P₄) / 4
  • Weighted: w₁P₁ + w₂P₂ + w₃P₃ + w₄P₄
    (weights = validation accuracy)
  • Stacking: Train meta-classifier on [P₁, P₂, P₃, P₄]
  ↓
Final Prediction: P(AI-generated)
```

---

## 4. Training Workflow

### 4.1 Experiment Pipeline

```
Phase 1: Baseline Establishment
  ↓
  Train each model independently:
    • Use same train/val/test split
    • Use same preprocessing
    • Grid search for hyperparameters
  ↓
  Record metrics for each model
  ↓
  Identify best baseline

Phase 2: Deep Dive on Best Performers
  ↓
  For top 2-3 models:
    • Perform ablation study (remove feature groups)
    • Test on special test sets (cross-domain, adversarial)
    • Analyze errors (confusion matrix deep dive)
  ↓
  Understand what signals each model uses

Phase 3: Ensemble Optimization
  ↓
  Test different ensemble combinations
  ↓
  Select final production model

Phase 4: Robustness Testing
  ↓
  Test on:
    • New LLM outputs (GPT-4.5, Claude-4, etc.)
    • Adversarially edited text
    • Mixed human-AI text
  ↓
  Identify failure modes
```

### 4.2 Hyperparameter Tuning Strategy

**Classical ML**:
- Method: Grid search or randomized search
- Cross-validation: 5-fold CV on training set
- Metrics: F1-score (balanced for any class imbalance)

**Neural Networks**:
- Method: Bayesian optimization or manual tuning
- Early stopping: Monitor validation loss, patience=5 epochs
- Learning rate scheduling: Reduce on plateau

**BERT**:
- Method: Manual tuning (expensive to grid search)
- Typical ranges:
  - Learning rate: {1e-5, 2e-5, 3e-5}
  - Batch size: {8, 16, 32}
  - Epochs: {3, 4, 5}
- Monitor: Validation accuracy + F1

---

## 5. Evaluation Framework

### 5.1 Metrics

**Primary Metrics**:
- **Accuracy**: Overall correctness
- **Precision**: Of texts flagged as AI, how many are truly AI?
  - High precision = low false positive rate (important for fairness)
- **Recall**: Of all AI texts, how many did we catch?
  - High recall = low false negative rate (important for detection)
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Discrimination ability across all thresholds

**Confusion Matrix Analysis**:
```
                Predicted
              Human    AI
Actual Human    TN     FP  ← False accusations (minimize this!)
       AI       FN     TP  ← Missed detections
```

**Special Focus**:
- False Positive Rate: Crucial for fairness (don't falsely accuse humans)
- False Negative Rate: Important for security (don't miss AI content)

### 5.2 Cross-Domain Evaluation

**Goal**: Test if model learns "AI vs human" or just "domain-specific" patterns

**Approach**:
```
Train on: Computer Science papers
  ↓
Test on:
  • CS papers (in-domain): Expect high performance
  • Biology papers (out-domain): Should still work well
  • Physics papers (out-domain)
  • Social sciences (out-domain)
  ↓
Compare performance drop:
  • Small drop (<5%): Model is robust
  • Large drop (>10%): Model overfit to domain
```

### 5.3 Adversarial Evaluation

**Test Case 1: Post-Edited AI Text**
- Take AI-generated text
- Have human edit it lightly (fix grammar, add personal touches)
- Test if model still detects AI origin

**Test Case 2: Grammar-Corrected Human Text**
- Take human text
- Run through Grammarly/LanguageTool
- Test if model false-flags it as AI

**Test Case 3: Hybrid Text**
- Human writes introduction
- AI completes the paragraph
- Test if model can detect mixed content

### 5.4 Interpretability Analysis

**For Classical ML**:
- **Feature importance**: Which features correlate with AI detection?
  - Example: High perplexity → human
  - Example: Uniform sentence length → AI
- **Weight analysis**: Which words/n-grams are strong signals?

**For BERT**:
- **Attention visualization**: Which tokens does model focus on?
- **SHAP values**: Contribution of each word to final prediction
- **Error analysis**: What patterns do misclassified samples share?

**Insights to Extract**:
- What linguistic patterns distinguish AI from human?
- Are there universal signals or domain-specific ones?
- Which LLMs are easiest/hardest to detect?

---

## 6. System Integration & Deployment (Future)

### 6.1 API Design

```
Input: Scientific text (string)
  ↓
Preprocessing
  ↓
Inference (all models)
  ↓
Ensemble prediction
  ↓
Output:
  • Probability: 0.0 (definitely human) to 1.0 (definitely AI)
  • Confidence interval
  • Explanation: Top contributing features/tokens
  • Warning: Model limitations
```

### 6.2 Monitoring Strategy

**Track over time**:
- Detection accuracy (as new LLMs emerge)
- False positive rate (fairness concern)
- Inference latency
- User feedback (if human appeals decision)

**Model Update Trigger**:
- Performance drops below threshold (e.g., <85% accuracy)
- New major LLM released (GPT-5, Claude-5, etc.)
- Significant false positive complaints

---

## 7. Limitations & Ethical Considerations

### 7.1 Technical Limitations

- **Not 100% accurate**: All models make mistakes
- **Adversarial attacks**: Sophisticated users can evade detection
- **Distribution shift**: Trained on 2020-2024 text, may not work on 2026+ style
- **Domain specificity**: Works best on academic text, may fail on creative writing

### 7.2 Ethical Considerations

- **False accusations**: Wrongly flagging human work as AI can harm reputation
  - Mitigation: High precision threshold (e.g., only flag if P(AI) > 0.9)
- **Fairness**: Non-native English speakers may be false-flagged more
  - Mitigation: Test on diverse author demographics
- **Transparency**: Users should know limitations
  - Mitigation: Clear documentation of accuracy, false positive rate
- **Use case restrictions**: Should NOT be sole evidence for academic punishment

### 7.3 Responsible Use Guidelines

- **Assistive tool, not judge**: Use as one signal among many
- **Human review required**: Always have expert review flagged cases
- **Appeal process**: Allow humans to explain their writing process
- **Privacy**: Don't store submitted text without consent

---

## 8. Success Metrics

**Technical Success**:
- Accuracy > 92% on held-out test set
- BERT outperforms classical ML by ≥5%
- Cross-domain performance drop < 5%
- False positive rate < 3%

**Research Success**:
- Identify 3-5 key linguistic signals of AI text
- Demonstrate ensemble improves over single models
- Publish findings on robustness to different LLMs

**Practical Success**:
- Deployable API with <2 second latency
- Explainable predictions (SHAP/attention maps)
- Documented limitations for end users

---

## 9. Timeline & Milestones

**Week 1-2**: Data Collection
- Crawl S2ORC, filter, clean
- Milestone: 100K human texts ready

**Week 3**: AI Text Generation
- Generate using multiple LLMs
- Milestone: 100K AI texts ready

**Week 4**: Data Analysis & Preprocessing
- EDA, feature engineering
- Milestone: Datasets split, features extracted

**Week 5-6**: Classical ML Training
- Train LR, SVM, RF, MLP
- Milestone: Baseline results documented

**Week 7-8**: BERT Fine-tuning
- Fine-tune BERT, SciBERT, DistilBERT
- Milestone: Best model identified

**Week 9**: Ensemble & Evaluation
- Build ensemble, run all evaluations
- Milestone: Final performance numbers

**Week 10**: Analysis & Documentation
- Error analysis, interpretability
- Milestone: Final report complete

---

## 10. Key Takeaways

**What Makes This System Effective**:
1. **High-quality data**: Realistic human text + diverse AI text
2. **Multi-model approach**: Classical ML for baseline, BERT for SOTA
3. **Rigorous evaluation**: Not just accuracy, but robustness and fairness
4. **Interpretability**: Understanding why model works

**Critical Success Factors**:
- Balanced dataset (domain, LLM sources)
- Proper train/test split (no data leakage)
- Comprehensive evaluation (cross-domain, adversarial)
- Ethical awareness (false positive mitigation)

**Future Extensions**:
- Multi-lingual support (non-English papers)
- Real-time adaptation (continual learning as new LLMs emerge)
- Multi-modal detection (check figures, citations, etc.)
- Collaborative filtering (detect AI across entire paper, not just abstract)